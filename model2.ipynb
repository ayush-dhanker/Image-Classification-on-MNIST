{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiM/x8LtZL9rns9kj94sBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush-dhanker/Image-Classification-on-MNIST/blob/main/model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VIxpPiQQOyIK"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing and transforming\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,))\n",
        "])\n",
        "\n",
        "# Loading Datasets\n",
        "train_dataset=datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "test_dataset=datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True, drop_last=True, num_workers=2)\n",
        "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False, num_workers=2)\n",
        "\n",
        "print(int(len(train_loader.dataset)/train_loader.batch_size))"
      ],
      "metadata": {
        "id": "xg3eR8CzPpAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd00597-98f5-4aaf-e214-fe322526481d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# building model\n",
        "model= nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(784,512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512,128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128,10)\n",
        ").to(device)\n",
        "\n",
        "print(\"model = \",model)\n",
        "with torch.no_grad():\n",
        "    print(\"Maximum weight before custom init: \", model[1].weight.max())\n",
        "\n",
        "\n",
        "def glorot_init(layer: nn.Module):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.apply(glorot_init)\n",
        "    print(\"Maximum weight after custom init\", model[1].weight.max())"
      ],
      "metadata": {
        "id": "YeMNG2PAP-_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b06eb29-0b51-4643-e339-35e5d26b1ce7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "model =  Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Maximum weight before custom init:  tensor(0.0357)\n",
            "Maximum weight after custom init tensor(0.0680)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.03)"
      ],
      "metadata": {
        "id": "-dxanqodQ8yB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    training_loader: DataLoader,\n",
        "    validation_loader: DataLoader,\n",
        "    n_epochs:int,\n",
        "    verbose:bool=True\n",
        "    ):\n",
        "\n",
        "\n",
        "\n",
        "  train_len=len(training_loader.dataset)\n",
        "  steps_per_epoch = train_len//training_loader.batch_size\n",
        "\n",
        "  print(\"Running {} epochs at {} steps per epoch \".format(n_epochs,steps_per_epoch))\n",
        "\n",
        "  train_acc=[]\n",
        "  train_loss=[]\n",
        "  val_acc=[]\n",
        "  val_loss=[]\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    if verbose:\n",
        "            print(\"Starting epoch {}...\".format(epoch + 1), end=\" \")\n",
        "\n",
        "    start_time = perf_counter()\n",
        "    epoch_train_loss=[]\n",
        "    epoch_train_acc=[]\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx,(input_batch,label_batch) in enumerate(training_loader):\n",
        "      batch_loss,batch_accuracy = training_set(input_batch,label_batch,model,loss_fn,optimizer)\n",
        "      epoch_train_loss.append(batch_loss.item())\n",
        "      epoch_train_acc.append(batch_accuracy.item())\n",
        "\n",
        "    end_time = perf_counter()\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    # evaluating\n",
        "    validation_loss, val_accuracy = evaluate(model,validation_loader,loss_fn)\n",
        "\n",
        "    val_acc.append(val_accuracy.item())\n",
        "    val_loss.append(validation_loss.item())\n",
        "    train_acc.append(np.mean(epoch_train_acc))\n",
        "    train_loss.append(np.mean(epoch_train_loss))\n",
        "\n",
        "    if verbose:\n",
        "            print(\"Time taken: {} seconds\".format(time_taken))\n",
        "            print(\"\\tTrain/val loss: {} / {}\".format(train_loss[-1], val_loss[-1]))\n",
        "            print(\"\\tTrain/val accuracy: {} / {}\".format(train_acc[-1], val_acc[-1]))\n",
        "\n",
        "  return {\"train_loss\": np.array(train_loss), \"train_acc\": np.array(train_acc),\n",
        "            \"val_loss\": np.array(val_loss), \"val_acc\": np.array(val_loss)}\n",
        "\n",
        "\n",
        "def training_set(\n",
        "      input: torch.tensor,\n",
        "      label: torch.tensor,\n",
        "      model: nn.Module,\n",
        "      loss_fn: nn.Module,\n",
        "      optimizer: optim.Optimizer):\n",
        "\n",
        "    input=input.to(device)\n",
        "    label=label.to(device)\n",
        "    output_batch=model(input)\n",
        "    loss_batch=loss_fn(output_batch, label)\n",
        "\n",
        "    loss_batch.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      batch_acc=accuracy(label,output_batch)\n",
        "    # see loss_batch.item()\n",
        "    return loss_batch, batch_acc\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "      model:nn.Module,\n",
        "      dataloader:DataLoader,\n",
        "      loss_fn:nn.Module):\n",
        "    model.eval()\n",
        "    size=len(dataloader.dataset)\n",
        "    num_batches=len(dataloader)\n",
        "    loss,correct=0,0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for input,label in dataloader:\n",
        "        input=input.to(device)\n",
        "        label=label.to(device)\n",
        "        prediction=model(input)\n",
        "        loss+=loss_fn(prediction,label)\n",
        "        correct += (prediction.argmax(axis=1)==label).type(torch.float).sum()\n",
        "\n",
        "    loss/=num_batches\n",
        "    val_accuracy=correct/size\n",
        "    return loss, val_accuracy\n",
        "\n",
        "def accuracy(labels:torch.tensor,\n",
        "               outputs:torch.tensor)->torch.tensor:\n",
        "               predictions=torch.argmax(outputs,axis=-1)\n",
        "               matches=labels==predictions\n",
        "               return matches.float().mean()"
      ],
      "metadata": {
        "id": "L_0XP60AOrhb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_model(model, loss_fn, optimizer, train_loader, test_loader, n_epochs=25)"
      ],
      "metadata": {
        "id": "TA6a9PLrQ76t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4d2910-6066-487e-9017-e8a3603452f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 25 epochs at 468 steps per epoch \n",
            "Starting epoch 1... Time taken: 14.004115915999591 seconds\n",
            "\tTrain/val loss: 0.008577954465360181 / 0.06210579350590706\n",
            "\tTrain/val accuracy: 0.9993656517094017 / 0.9807000160217285\n",
            "Starting epoch 2... Time taken: 13.907658698999967 seconds\n",
            "\tTrain/val loss: 0.00771361013996797 / 0.06166772544384003\n",
            "\tTrain/val accuracy: 0.9995993589743589 / 0.9812999963760376\n",
            "Starting epoch 3... Time taken: 14.138782378000087 seconds\n",
            "\tTrain/val loss: 0.007134221703894675 / 0.062419354915618896\n",
            "\tTrain/val accuracy: 0.9996494391025641 / 0.9817000031471252\n",
            "Starting epoch 4... Time taken: 14.417706552999789 seconds\n",
            "\tTrain/val loss: 0.00658716739460818 / 0.06260804831981659\n",
            "\tTrain/val accuracy: 0.999732905982906 / 0.9811000227928162\n",
            "Starting epoch 5... Time taken: 13.9489416790002 seconds\n",
            "\tTrain/val loss: 0.006131560179259644 / 0.06264001131057739\n",
            "\tTrain/val accuracy: 0.9997662927350427 / 0.98089998960495\n",
            "Starting epoch 6... Time taken: 13.859612996999658 seconds\n",
            "\tTrain/val loss: 0.005668919103152445 / 0.06285055726766586\n",
            "\tTrain/val accuracy: 0.9997996794871795 / 0.9807999730110168\n",
            "Starting epoch 7... Time taken: 14.144185358999948 seconds\n",
            "\tTrain/val loss: 0.0053298948374954965 / 0.06291444599628448\n",
            "\tTrain/val accuracy: 0.9998330662393162 / 0.9817000031471252\n",
            "Starting epoch 8... Time taken: 13.862973583999974 seconds\n",
            "\tTrain/val loss: 0.004890168382702634 / 0.06347565352916718\n",
            "\tTrain/val accuracy: 0.9999165331196581 / 0.9812999963760376\n",
            "Starting epoch 9... Time taken: 14.03497429800018 seconds\n",
            "\tTrain/val loss: 0.004626371953850334 / 0.06405335664749146\n",
            "\tTrain/val accuracy: 0.9998998397435898 / 0.9819999933242798\n",
            "Starting epoch 10... Time taken: 13.86177504599982 seconds\n",
            "\tTrain/val loss: 0.0043547682692475905 / 0.06283733993768692\n",
            "\tTrain/val accuracy: 0.9999499198717948 / 0.9817000031471252\n",
            "Starting epoch 11... Time taken: 14.262300070000038 seconds\n",
            "\tTrain/val loss: 0.00406151274296268 / 0.06341582536697388\n",
            "\tTrain/val accuracy: 0.9999499198717948 / 0.9814000129699707\n",
            "Starting epoch 12... Time taken: 13.826222544000302 seconds\n",
            "\tTrain/val loss: 0.003870230998433967 / 0.06332865357398987\n",
            "\tTrain/val accuracy: 0.9999499198717948 / 0.9821000099182129\n",
            "Starting epoch 13... Time taken: 13.819750842999838 seconds\n",
            "\tTrain/val loss: 0.003629212767063863 / 0.06353204697370529\n",
            "\tTrain/val accuracy: 0.9999666132478633 / 0.9818999767303467\n",
            "Starting epoch 14... Time taken: 13.969217087000288 seconds\n",
            "\tTrain/val loss: 0.003448091168006341 / 0.06430324912071228\n",
            "\tTrain/val accuracy: 0.9999499198717948 / 0.9811999797821045\n",
            "Starting epoch 15... Time taken: 14.203927890000159 seconds\n",
            "\tTrain/val loss: 0.003273111143329531 / 0.06394490599632263\n",
            "\tTrain/val accuracy: 0.9999499198717948 / 0.9819999933242798\n",
            "Starting epoch 16... Time taken: 13.95627942100009 seconds\n",
            "\tTrain/val loss: 0.003119689288537185 / 0.06390360742807388\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9821000099182129\n",
            "Starting epoch 17... Time taken: 13.91931987199996 seconds\n",
            "\tTrain/val loss: 0.0029759346826512437 / 0.06395775824785233\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9815999865531921\n",
            "Starting epoch 18... Time taken: 13.947809210999822 seconds\n",
            "\tTrain/val loss: 0.0028323462662291038 / 0.06445485353469849\n",
            "\tTrain/val accuracy: 0.9999666132478633 / 0.9818999767303467\n",
            "Starting epoch 19... Time taken: 14.007864036000228 seconds\n",
            "\tTrain/val loss: 0.00271661667791036 / 0.06481225043535233\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9818999767303467\n",
            "Starting epoch 20... Time taken: 13.907432305999919 seconds\n",
            "\tTrain/val loss: 0.002602697014146978 / 0.06521610915660858\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9812999963760376\n",
            "Starting epoch 21... Time taken: 13.855388052000308 seconds\n",
            "\tTrain/val loss: 0.0024909490438846825 / 0.06476613879203796\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9819999933242798\n",
            "Starting epoch 22... Time taken: 13.801678939999874 seconds\n",
            "\tTrain/val loss: 0.0023905782399795605 / 0.06531097739934921\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9815000295639038\n",
            "Starting epoch 23... Time taken: 13.88586336499975 seconds\n",
            "\tTrain/val loss: 0.0023047248259511115 / 0.06539963185787201\n",
            "\tTrain/val accuracy: 1.0 / 0.9818000197410583\n",
            "Starting epoch 24... Time taken: 13.893240311000227 seconds\n",
            "\tTrain/val loss: 0.002216745361436505 / 0.06520242244005203\n",
            "\tTrain/val accuracy: 0.9999833066239316 / 0.9815999865531921\n",
            "Starting epoch 25... Time taken: 13.841067805999955 seconds\n",
            "\tTrain/val loss: 0.0021334187494308297 / 0.06584557145833969\n",
            "\tTrain/val accuracy: 1.0 / 0.9819999933242798\n"
          ]
        }
      ]
    }
  ]
}