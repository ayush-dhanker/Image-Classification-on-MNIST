{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlN5BtDwZ72KrimX8ty1jm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush-dhanker/Image-Classification-on-MNIST/blob/main/model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VIxpPiQQOyIK"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing and transforming\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,))\n",
        "])\n",
        "\n",
        "# Loading Datasets\n",
        "train_dataset=datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "test_dataset=datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True, drop_last=True, num_workers=5)\n",
        "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "xg3eR8CzPpAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5893cb84-5a8a-4482-d24c-6bd3a1a47f48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.04MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 65.9kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.28MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.32MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# building model\n",
        "model= nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(784,512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512,128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128,10)\n",
        ").to(device)\n",
        "\n",
        "print(\"model = \",model)\n",
        "with torch.no_grad():\n",
        "    print(\"Maximum weight before custom init: \", model[1].weight.max())\n",
        "\n",
        "\n",
        "def glorot_init(layer: nn.Module):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.apply(glorot_init)\n",
        "    print(\"Maximum weight after custom init\", model[1].weight.max())"
      ],
      "metadata": {
        "id": "YeMNG2PAP-_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a9f826-d87c-41cd-8881-1a287dd85435"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "model =  Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (4): ReLU()\n",
            "  (5): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "Maximum weight before custom init:  tensor(0.0357, device='cuda:0')\n",
            "Maximum weight after custom init tensor(0.0680, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(), lr=0.03)"
      ],
      "metadata": {
        "id": "-dxanqodQ8yB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    loss_fn: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    training_loader: DataLoader,\n",
        "    validation_loader: DataLoader,\n",
        "    n_epochs:int,\n",
        "    verbose:bool=True\n",
        "    ):\n",
        "\n",
        "\n",
        "\n",
        "  train_len=len(training_loader.dataset)\n",
        "  steps_per_epoch = train_len/training_loader.batch_size\n",
        "\n",
        "  print(\"Running {} epochs at {} steps per epoch \".format(n_epochs,steps_per_epoch))\n",
        "\n",
        "  train_acc=[]\n",
        "  train_loss=[]\n",
        "  val_acc=[]\n",
        "  val_loss=[]\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    if verbose:\n",
        "            print(\"Starting epoch {}...\".format(epoch + 1), end=\" \")\n",
        "\n",
        "    start_time = perf_counter()\n",
        "    epoch_train_loss=[]\n",
        "    epoch_train_acc=[]\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx,(input_batch,label_batch) in enumerate(training_loader):\n",
        "      batch_loss,batch_accuracy = training_set(input_batch,label_batch,model,loss_fn,optimizer)\n",
        "      epoch_train_loss.append(batch_loss.item())\n",
        "      epoch_train_acc.append(batch_accuracy.item())\n",
        "\n",
        "    end_time = perf_counter()\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    # evaluating\n",
        "    validation_loss, val_accuracy = evaluate(model,validation_loader,loss_fn)\n",
        "\n",
        "    val_acc.append(val_accuracy.item())\n",
        "    val_loss.append(validation_loss.item())\n",
        "    train_acc.append(np.mean(epoch_train_acc))\n",
        "    train_loss.append(np.mean(epoch_train_loss))\n",
        "\n",
        "    if verbose:\n",
        "            print(\"Time taken: {} seconds\".format(time_taken))\n",
        "            print(\"\\tTrain/val loss: {} / {}\".format(train_loss[-1], val_loss[-1]))\n",
        "            print(\"\\tTrain/val accuracy: {} / {}\".format(train_acc[-1], val_acc[-1]))\n",
        "\n",
        "  return {\"train_loss\": np.array(train_loss), \"train_acc\": np.array(train_acc),\n",
        "            \"val_loss\": np.array(val_loss), \"val_acc\": np.array(val_loss)}\n",
        "\n",
        "\n",
        "def training_set(\n",
        "      input: torch.tensor,\n",
        "      label: torch.tensor,\n",
        "      model: nn.Module,\n",
        "      loss_fn: nn.Module,\n",
        "      optimizer: optim.Optimizer):\n",
        "\n",
        "    input=input.to(device)\n",
        "    label=label.to(device)\n",
        "    output_batch=model(input)\n",
        "    loss_batch=loss_fn(output_batch, label)\n",
        "\n",
        "    loss_batch.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      batch_acc=accuracy(output_batch,label)\n",
        "    # see loss_batch.item()\n",
        "    return loss_batch,batch_acc\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "      model:nn.Module,\n",
        "      dataloader:DataLoader,\n",
        "      loss_fn:nn.Module):\n",
        "    model.eval()\n",
        "    size=len(dataloader.dataset)\n",
        "    num_batches=len(dataloader)\n",
        "    loss=0\n",
        "    correct=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for input,label in dataloader:\n",
        "        input=input.to(device)\n",
        "        label=label.to(device)\n",
        "        prediction=model(input)\n",
        "        loss+=loss_fn(prediction,label)\n",
        "        correct += (prediction.argmax(axis=1)==label).type(torch.float).sum().item()\n",
        "\n",
        "    loss/=num_batches\n",
        "    val_accuracy=correct/size\n",
        "    return loss, val_accuracy\n",
        "\n",
        "def accuracy(labels:torch.tensor,\n",
        "               outputs:torch.tensor)->torch.tensor:\n",
        "               predictions=torch.argmax(outputs,axis=-1)\n",
        "               matches=labels==predictions\n",
        "               return matches.float().mean()"
      ],
      "metadata": {
        "id": "L_0XP60AOrhb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_model(model, loss_fn, optimizer, train_loader, test_loader, n_epochs=25)"
      ],
      "metadata": {
        "id": "TA6a9PLrQ76t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "70e90a3e-b885-468b-9bbd-7ffbd4178f1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 25 epochs at 468.75 steps per epoch \n",
            "Starting epoch 1... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'float' object has no attribute 'item'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fe9a6e69b94d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-1f76d158d268>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_fn, optimizer, training_loader, validation_loader, n_epochs, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
          ]
        }
      ]
    }
  ]
}